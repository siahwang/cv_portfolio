{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford CS231n Deep Learning for Computer Vision\n",
    "#### Module 1. Neural Networks: Linear Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function implementation\n",
    "# unvectorized ver. \n",
    "\n",
    "def L_i(x,y,W):\n",
    "    \"\"\"\n",
    "    unvectorized version. Compute the multiclass svm loss for a single example (x,y)\n",
    "    - x: a column vector representing an image (e.g. 3073*1 in CIFAR-10) with an appended bias dimension in the 3073-rd position (i.e. bias trick)\n",
    "    - y: an integer giving index of correct class\n",
    "    - W: the weight matrix (e.g. 10*3073 in CIFAR-10)\n",
    "    \"\"\"\n",
    "    delta: 1.0 \n",
    "    scores=W.dot(x) # scores become of size 10*1, the scores for each class\n",
    "    correct_class_score=scores[y]\n",
    "    D=W.shape[0] # number of classes, e.g. 10\n",
    "    loss_i=0.0\n",
    "    \n",
    "    for j in range(D): #iterate over all wrong classes\n",
    "        if j==y:\n",
    "            continue # skipping for the true class to only loop over incorrect classes\n",
    "        loss_i+=max(0, scores[j]-correct_class_score+delta)\n",
    "        return loss_i\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Half-vectorized ver. \n",
    "def L_i_vectorized(x,y,W):\n",
    "    \"\"\" \n",
    "    A faster half-vectorized implementation. half-vectorized refers to the fact that for a single example the implementation contains no for loops, but there is still one loop over the examples (outside this function)\n",
    "    \"\"\" \n",
    "    delta=1.0\n",
    "    scores=W.dot(x)\n",
    "    # compute the margins for all calsses in one vector operation\n",
    "    margins=np.maximum(0, scores-scores[y]+delta) \n",
    "    # on y-th position scores[y]-scores[y] canceld and gave delta. We want to ignore the y-th position and only consider margin on max wrong class\n",
    "    margins[y]=0\n",
    "    loss_i=np.sum(margins)\n",
    "    return loss_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "##### Strategy 1: A first very bad idea solution: Random search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train: the data where each column is a example (e.g. 3073(pixels as rows) * 50,000(images as cols))\n",
    "# Y_train: the labels (e.g. 1D array of 50,000(ground truth labels))\n",
    "# L: evaluates the loss function\n",
    "# W: weight matrix (10(# of class)*3073(features)) \n",
    "# each row of W corresponds to the template of each class(Wk)\n",
    "import numpy as np\n",
    "\n",
    "bestloss=float(\"inf\")\n",
    "for num in range(1000):\n",
    "    W=np.random.randn(10, 3073)*0.0001 # generate random parameters for each iteration\n",
    "    loss= L_i(X_train, Y_train, W) # get the loss over the entire training set \n",
    "    if loss<bestloss: \n",
    "        bestloss=loss\n",
    "        bestW=W\n",
    "        \n",
    "        print(\"in attempt %d the loss was %f, best %f\" %(num, loss, bestloss))\n",
    "        \n",
    "# Assume X_test is [3073 x 10000], Y_test [10000 x 1]\n",
    "scores = bestW.dot(Xte_cols) # 10 x 10000, the class scores for all test examples\n",
    "# find the index with max score in each col (the predicted class)\n",
    "Yte_predict = np.argmax(scores, axis=0)\n",
    "# and calculate accuracy (fraction of predictions that are correct)\n",
    "np.mean(Yte_predict==Yte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Strategy 2: Random Local Search \n",
    "##### Start with random weights and iteratively refine them over time to get lower loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W=np.random.randn(10,3073)*0.001 # generate random starting W \n",
    "bestloss=float(\"inf\")\n",
    "for i in range(1000):\n",
    "    step_size=0.0001\n",
    "    Wtry=W+np.random.randn(10,3073)*step_size\n",
    "    loss=L_i(X_train, Y_train, Wtry)\n",
    "    if loss<bestloss:\n",
    "        W=Wtry\n",
    "        bestloss=loss\n",
    "    print(\"iter %d loss is %f\" % (i, bestloss))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy 3: Following the Gradient \n",
    "\n",
    "There are two ways to compute the gradient:\n",
    "1) Numerical gradient: A slow, approximate but easy way\n",
    "2) Analytic gradient: A fast, exact but more error-prone way that requires calculus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the gradient numerically with finite differences\n",
    "\n",
    "def eval_numerical_gradient(f,x):\n",
    "    \"\"\"\n",
    "    a naive implementation of numerical gradient of f @ x\n",
    "    - f should be a function that takes a single argument\n",
    "    - x is the point (numpy array) to evaluate the gradient at \n",
    "    \"\"\"\n",
    "    \n",
    "    fx=f(x) # evaluate function value at original point\n",
    "    grad=np.zeros(x.shape)\n",
    "    h=0.00001\n",
    "    \n",
    "    #iterate over all indexes in x\n",
    "    it=np.nditer(x, flags=[\"multi_index\"], op_flags=[\"readwrite\"])\n",
    "    while not it.finished: \n",
    "        # evaluate function at x+h\n",
    "        ix=it.multi_index\n",
    "        old_value=x[ix]\n",
    "        x[ix]=old_value+h\n",
    "        fxh=f(x) # evaluate f(x+h)\n",
    "        x[ix]=old_value #restore to previous value *****\n",
    "        \n",
    "        #compute the partial derivative \n",
    "        grad[ix]=(fxh-fx)/h\n",
    "        it=iternext()\n",
    "        \n",
    "    return grad\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The gradient tells us **which direction** decreases loss fastest, but not **how far to move(step size)**.\n",
    "- A small step size yields steady but slow progress; a large step can overshoot and worsen loss.\n",
    "- Finite‐difference gradients take O(D) loss evaluations (one per parameter) and don’t scale to millions of weights.\n",
    "- In practice, we rely on efficient automatic differentiation (autograd) instead of numeric gradients.\n",
    "\n",
    "- Analytic gradient computation is exact and easy to implement once you’ve derived the formulas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla Gradient Descent \n",
    "\n",
    "while True:\n",
    "    weights_grad=evaluate_gradient(loss_ftn, data, weights)\n",
    "    weights+= -step_size*weights_grad # perform parameter update --> towards negative gradient direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla Minibatch Gradient Descent\n",
    "# In practice, the dataset would not contain duplicate images, the gradient from a mini-batch is a good approximation of the gradient of the full objective.\n",
    "while True: \n",
    "    data_batch=sample_training_data(data, 256) #sample 256 examples\n",
    "    weights_grad=evaluate_gradient(loss_ftn, data_batch, weights)\n",
    "    weights+= -step_size*weights_grad "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section Summary\n",
    "\n",
    "In this section, we covered the end-to-end process of optimizing a high-dimensional loss landscape:\n",
    "\n",
    "1. **Loss as a “Bowl-Shaped” Landscape**  \n",
    "   - We view the loss function as a high-dimensional terrain and our goal as finding its bottom.  \n",
    "   - The SVM loss is piece-wise linear but globally bowl-shaped.\n",
    "\n",
    "2. **Iterative Refinement**  \n",
    "   - Start from a random weight initialization.  \n",
    "   - Gradually refine the weights step by step to reduce the loss.\n",
    "\n",
    "3. **Role of the Gradient**  \n",
    "   - The gradient points in the direction of steepest increase.  \n",
    "   - To minimize loss, we move in the **negative gradient** direction.\n",
    "\n",
    "4. **Numerical vs. Analytic Gradients**  \n",
    "   - **Numerical gradients** use finite differences \\((f(x+h) - f(x)) / h\\): simple but \\(O(D)\\) expensive.  \n",
    "   - **Analytic gradients** are exact and efficient but require correct mathematical derivation.  \n",
    "   - In practice we implement the analytic form and perform a **gradient check** against the numerical result.\n",
    "\n",
    "5. **Importance of Step Size (Learning Rate)**  \n",
    "   - Too small ⇒ steady but slow convergence.  \n",
    "   - Too large ⇒ risk of overshooting or divergence.  \n",
    "   - Choosing and scheduling the right learning rate is critical.\n",
    "\n",
    "6. **Vanilla Gradient Descent Loop**  \n",
    "   ```python\n",
    "   while not converged:\n",
    "       grad = compute_gradient(loss_fn, data, weights)\n",
    "       weights -= learning_rate * grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
