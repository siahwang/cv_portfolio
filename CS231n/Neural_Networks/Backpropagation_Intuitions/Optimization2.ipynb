{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS231n\n",
    "#### Optimization 2\n",
    "## Intuitive Summary of Partial Derivatives\n",
    "\n",
    "\n",
    "### 1. Definition of a Partial Derivative\n",
    "\n",
    "A partial derivative measures how a function changes as **one variable** changes, holding all others constant:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f(x)}{\\partial x}\n",
    "= \\lim_{h \\to 0} \\frac{f(x + h) - f(x)}{h}\n",
    "$$\n",
    "\n",
    "- **Intuition**  \n",
    "  - If you increase \\(x\\) by a tiny amount \\(h\\), the partial derivative tells you **how much** and **in which direction** \\(f\\) will change.  \n",
    "  - **Sign**: positive means \\(f\\) increases, negative means \\(f\\) decreases.  \n",
    "  - **Magnitude**: the sensitivity of \\(f\\) to \\(x\\).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Addition Function\n",
    "\n",
    "Consider\n",
    "\n",
    "$$\n",
    "f(x,y) = x + y.\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x} = 1, \n",
    "\\quad\n",
    "\\frac{\\partial f}{\\partial y} = 1.\n",
    "$$\n",
    "\n",
    "- **Intuition**  \n",
    "  - Bumping either \\(x\\) or \\(y\\) by \\(h\\) raises \\(f\\) by exactly \\(h\\).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Max Function (Subgradient)\n",
    "\n",
    "Consider\n",
    "\n",
    "$$\n",
    "f(x,y) = \\max(x,y).\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial x}\n",
    "=\n",
    "\\begin{cases}\n",
    "1, & x \\ge y,\\\\\n",
    "0, & x < y,\n",
    "\\end{cases}\n",
    "\\quad\n",
    "\\frac{\\partial f}{\\partial y}\n",
    "=\n",
    "\\begin{cases}\n",
    "1, & y \\ge x,\\\\\n",
    "0, & y < x.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- **Intuition**  \n",
    "  - Only the larger input “wins” and gets gradient 1; the other gets 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward & Backward Pass: End-to-End Flow\n",
    "\n",
    "### 1. Forward Pass\n",
    "1. **Inputs**  \n",
    "   \\(x, y, z\\)  (예: \\(x=-2,\\;y=5,\\;z=-4\\))\n",
    "2. **Step 1: Addition**  \n",
    "   $$\n",
    "   q = x + y\n",
    "   \\quad\\Longrightarrow\\quad\n",
    "   q = -2 + 5 = 3\n",
    "   $$\n",
    "3. **Step 2: Multiplication**  \n",
    "   $$\n",
    "   f = q \\times z\n",
    "   \\quad\\Longrightarrow\\quad\n",
    "   f = 3 \\times (-4) = -12\n",
    "   $$\n",
    "4. **Output**  \n",
    "   \\(f = -12\\)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Backward Pass\n",
    "1. **Start at the output**  \n",
    "   $$\n",
    "   \\frac{\\partial f}{\\partial f} = 1\n",
    "   $$\n",
    "2. **Backprop through** \\(f = q \\times z\\)  \n",
    "   $$\n",
    "   \\frac{\\partial f}{\\partial q} = z = -4,\n",
    "   \\quad\n",
    "   \\frac{\\partial f}{\\partial z} = q = 3\n",
    "   $$\n",
    "3. **Backprop through** \\(q = x + y\\)  \n",
    "   $$\n",
    "   \\frac{\\partial q}{\\partial x} = 1,\n",
    "   \\quad\n",
    "   \\frac{\\partial q}{\\partial y} = 1\n",
    "   $$\n",
    "4. **Chain rule 적용**  \n",
    "   $$\n",
    "   \\frac{\\partial f}{\\partial x}\n",
    "   = \\frac{\\partial f}{\\partial q}\\,\\frac{\\partial q}{\\partial x}\n",
    "   = (-4)\\times1 = -4,\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial f}{\\partial y}\n",
    "   = \\frac{\\partial f}{\\partial q}\\,\\frac{\\partial q}{\\partial y}\n",
    "   = (-4)\\times1 = -4,\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial f}{\\partial z}\n",
    "   = 3\n",
    "   $$\n",
    "5. **Gradients 결과**  \n",
    "   $$\n",
    "   \\frac{\\partial f}{\\partial x} = -4,\n",
    "   \\quad\n",
    "   \\frac{\\partial f}{\\partial y} = -4,\n",
    "   \\quad\n",
    "   \\frac{\\partial f}{\\partial z} = 3.\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Goal of Backpropagation\n",
    "- **Objective**: Compute how **sensitive** the loss function \\(L\\) is to each parameter \\(w_i\\), i.e.\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial w_i}.\n",
    "  $$\n",
    "- **How**:\n",
    "  1. Define **local derivatives** at each node in the computation graph.  \n",
    "  2. From the final loss, **propagate gradients backward** using the **chain rule**.  \n",
    "  3. Accumulate each parameter’s gradient in turn.\n",
    "\n",
    "This entire reverse-gradient process is called **backpropagation**, and calling `loss.backward()` in a deep learning framework **automates** it.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Key Takeaway\n",
    "- **Forward**: Compute values from inputs to loss via each operation.  \n",
    "- **Backward**: Starting at loss, propagate gradients backward through each operation node by node.  \n",
    "- **Backpropagation** = chaining local gradients via the chain rule to obtain \\(\\frac{\\partial L}{\\partial w_i}\\) for every parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Derivatives & Chain Rule\n",
    "\n",
    "### 1. Local derivative “formulas”\n",
    "Each operation (gate) knows how to compute its own partial derivatives:\n",
    "\n",
    "- **Addition**  \n",
    "  $q = x + y$  \n",
    "  $ \\partial q / \\partial x = 1 $,  \n",
    "  $ \\partial q / \\partial y = 1 $\n",
    "\n",
    "- **Multiplication**  \n",
    "  $f = q \\times z$  \n",
    "  $ \\partial f / \\partial q = z $,  \n",
    "  $ \\partial f / \\partial z = q $\n",
    "\n",
    "- **ReLU**  \n",
    "  $g(z) = \\max(0, z)$  \n",
    "  $ g'(z) = 1 $ if $z>0$, else $g'(z) = 0$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Chain Rule\n",
    "To compute the gradient of a composite function, multiply local derivatives along the path:\n",
    "\n",
    "If $f(x,y,z) = (x + y)\\,z$, then\n",
    "\n",
    "- $ \\partial f / \\partial x = (\\partial f / \\partial q)\\times(\\partial q/\\partial x) = z \\times 1 $\n",
    "- $ \\partial f / \\partial y = z \\times 1 $\n",
    "- $ \\partial f / \\partial z = q \\times 1 $\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Backpropagation = Chain Rule on the Graph\n",
    "1. Define local derivatives at each node.  \n",
    "2. Start from the loss output and propagate gradients backward by multiplying local derivatives at each step.  \n",
    "3. Frameworks’ `loss.backward()` automates this process across the entire network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Neuron: Forward & Backward Setup\n",
    "\n",
    "### 1. Neuron definition\n",
    "- **Weights**: $w = [w_0, w_1, w_2]$ where $w_0$ is the bias  \n",
    "- **Inputs**: $x = [x_0, x_1]$  \n",
    "\n",
    "The neuron computes:\n",
    "1. **Affine (dot) layer**  \n",
    "   $$\n",
    "     \\text{dot} \\;=\\; w_0\\,x_0 \\;+\\; w_1\\,x_1 \\;+\\; w_2\n",
    "   $$\n",
    "2. **Sigmoid activation**  \n",
    "   $$\n",
    "     f \\;=\\; \\sigma(\\text{dot})\n",
    "     \\;=\\;\n",
    "     \\frac{1}{1 + e^{-\\text{dot}}}\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Forward pass example\n",
    "Let\n",
    "\\[\n",
    "  w = [2,\\,-3,\\,-3], \\quad\n",
    "  x = [-1,\\,-2].\n",
    "\\]\n",
    "\n",
    "1. **Compute dot**  \n",
    "   $$\n",
    "     \\text{dot}\n",
    "     = 2\\times(-1) \\;+\\; (-3)\\times(-2) \\;+\\; (-3)\n",
    "     = -2 \\;+\\; 6 \\;-\\; 3\n",
    "     = 1.\n",
    "   $$\n",
    "2. **Apply sigmoid**  \n",
    "   $$\n",
    "     f = \\frac{1}{1 + e^{-1}}\n",
    "       \\approx 0.731.\n",
    "   $$\n",
    "\n",
    "In code:\n",
    "```python\n",
    "dot = w[0]*x[0] + w[1]*x[1] + w[2]   # dot = 1\n",
    "f   = 1.0 / (1 + math.exp(-dot))    # f ≈ 0.731\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The backprop for this neuron\n",
    "w=[2,-3,-3] \n",
    "x=[-1,-2]\n",
    "\n",
    "# forward pass\n",
    "dot=w[0]*x[0]+w[1]*x[1]+w[2]\n",
    "f=1.0/(1+math.exp(-dot)) # sigmoid function\n",
    "\n",
    "# backward pass through this neuron (backpropagation)\n",
    "ddot=f*(1-f) # gradient on dot variable(d dot/d), using the sigmoid gradient derivation\n",
    "dx=[w[0]*ddot, w[1]*ddot]\n",
    "dw=[x[0]*ddot, x[1]*ddot, 1.0*ddot]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop in Practice: Staged Computation\n",
    "\n",
    "Consider the function\n",
    "$$\n",
    "  f(x, y) \\;=\\; \\frac{x + \\sigma(y)}{\\;\\sigma(x)\\;+\\;(x + y)^2\\;},\n",
    "$$\n",
    "where $\\sigma(z)=1/(1+e^{-z})$ is the sigmoid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5456448841066441\n"
     ]
    }
   ],
   "source": [
    "x = 3 # example values\n",
    "y = -4\n",
    "\n",
    "# forward pass\n",
    "sigy = 1.0 / (1 + math.exp(-y)) # sigmoid in numerator   #(1)\n",
    "num = x + sigy # numerator                               #(2)\n",
    "sigx = 1.0 / (1 + math.exp(-x)) # sigmoid in denominator #(3)\n",
    "xpy = x + y                                              #(4)\n",
    "xpysqr = xpy**2                                          #(5)\n",
    "den = sigx + xpysqr # denominator                        #(6)\n",
    "invden = 1.0 / den                                       #(7)\n",
    "f = num * invden # done                                  #(8)\n",
    "\n",
    "\n",
    "# backprop \n",
    "# --- Stage 8 → 7: f = num * invden ---\n",
    "# 1) f 에서 num 과 invden 에 각각 기울기 전달\n",
    "dnum    = invden      # ∂f/∂num   = invden\n",
    "dinvden = num         # ∂f/∂invden = num\n",
    "\n",
    "# --- Stage 7 → 6: invden = 1/den ---\n",
    "# 2) invden 에서 den 으로 기울기 전파\n",
    "#    ∂(1/den)/∂den = -1/den²\n",
    "\n",
    "dden    = (-1.0 / (den**2)) * dinvden\n",
    "# --- Stage 6 → 5,3: den = sigx + xpysqr ---\n",
    "# 3) 덧셈이므로, sigx 와 xpysqr 에 똑같이 dden 전파\n",
    "dsigx   = 1 * dden\n",
    "dxpysqr = 1 * dden\n",
    "\n",
    "# --- Stage 5 → 4: xpysqr = xpy**2 ---\n",
    "# 4) 제곱의 국소 미분: ∂(xpy²)/∂xpy = 2*xpy\n",
    "dxpy    = (2 * xpy) * dxpysqr\n",
    "\n",
    "# --- Stage 4 → 2,4: xpy = x + y ---\n",
    "# 5) 덧셈이므로, x 와 y 에 똑같이 dxpy 전파\n",
    "dx      = 1 * dxpy\n",
    "dy      = 1 * dxpy\n",
    "\n",
    "# --- Stage 3 → x: sigx = σ(x) ---\n",
    "# 6) 시그모이드의 국소 미분: ∂σ/∂x = σ(x)*(1−σ(x))\n",
    "#    이미 dx 에 xpy 경로 기울기가 들어 있으니 += 로 누적\n",
    "dx     += ((1 - sigx) * sigx) * dsigx\n",
    "\n",
    "# --- Stage 2 → x,y: num = x + sigy ---\n",
    "# 7) num = x + sigy 인 덧셈 블록\n",
    "dx     += 1 * dnum     # ∂num/∂x    = 1\n",
    "dsigy   = 1 * dnum     # ∂num/∂sigy = 1\n",
    "\n",
    "# --- Stage 1 → y: sigy = σ(y) ---\n",
    "# 8) 마지막으로 시그모이드 국소 미분 전파\n",
    "dy     += ((1 - sigy) * sigy) * dsigy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix-Matrix multiply gradient\n",
    "\n",
    "# forward pass\n",
    "W=np.random.randn(5,10)  # (5 x 10)=weights\n",
    "X=np.random.randn(10,3)  # (10 x 3)=inputs/activations\n",
    "D=W.dot(X)\n",
    "\n",
    "# backward pass (given dD = ∂L/∂D)\n",
    "dD=np.random.randn(*D.shape)\n",
    "\n",
    "dW=dD.dot(X.T)   # (5×3)·(3×10) → (5×10)\n",
    "dX = W.T.dot(dD) # (10×5)·(5×3) → (10×3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
